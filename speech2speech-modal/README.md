# Speech-to-Speech Pipeline

Production-ready speech-to-speech system on Modal. Takes audio, transcribes it, generates a response, and synthesizes speechâ€”all in-memory, GPU-accelerated.

**Audio** â†’ **ASR** â†’ **Text** â†’ **LLM** â†’ **Response** â†’ **TTS** â†’ **Audio**

## Features

âœ… **Modular architecture**: Base classes + concrete implementations for easy model swapping  
âœ… **Config-based model selection**: Choose models via environment variables  
âœ… **Audio compression**: MP3 compression for optimized network transfer  
âœ… **GPU-accelerated**: A10G GPUs for all components  
âœ… **In-memory processing**: No temporary files  
âœ… **Low latency**: 3-7 seconds E2E (warm)  
âœ… **Production-ready**: Error handling, logging, monitoring, metrics tracking  
âœ… **Real-time VAD client**: Voice Activity Detection with silence detection  
âœ… **Easy to extend**: Add new ASR/LLM/TTS implementations in 3 steps  
âœ… **Modal deployment**: Serverless, auto-scaling  

## Models

| Component | Model | Provider | Size |
|-----------|-------|----------|------|
| **ASR** | nvidia/nemotron-speech-streaming-en-0.6b | NeMo | 600M params |
| **LLM** | microsoft/Phi-3-mini-4k-instruct | Hugging Face | 3.8B params |
| **TTS** | ChatterboxTTS (turbo) | Hugging Face | ~250M params |

## Quick Start

### 1. Install

```bash
pip install -r requirements.txt
modal token new  # Authenticate with Modal
```

### 2. Test Locally

```bash
# Local testing with your own WAV file
modal run modular_main.py --audio-path input.wav
```

Output: `output.wav`

### 3. Deploy to Modal (Default Models)

```bash
# Deploy with default models (NeMo ASR, Phi3 LLM, ChatterboxTTS)
modal deploy modular_main.py
```

### 4. Deploy with Custom Models

```bash
# Deploy with custom models via environment variables
$env:ASR_MODEL="nemo"; $env:LLM_MODEL="phi3"; $env:TTS_MODEL="chatterbox"; modal deploy modular_main.py
```

Supported models:
- **ASR**: `nemo`, `whisper`
- **LLM**: `phi3`, `llama`, `gpt4omini`
- **TTS**: `chatterbox`

### 5. Use Real-Time Client

```bash
# Connect with voice activity detection and real-time interaction
python client.py
```

Features:
- Live microphone input with VAD (Voice Activity Detection)
- Automatic silence detection (stops after 1 second of silence)
- Compression for optimized network transfer
- Session metrics tracking (latency, throughput)

### 6. Call Deployed Service Programmatically

```python
from modal import Function
from audio_compression import compress_wav_to_mp3, decompress_mp3_to_wav

f = Function.lookup("speech-to-speech", "process_audio")

with open("input.wav", "rb") as f:
    audio_bytes = f.read()

# Optional: compress audio for faster transfer
compressed = compress_wav_to_mp3(audio_bytes, bitrate=64)
result = f.remote(compressed)

# Decompress output
output_audio = decompress_mp3_to_wav(result)

with open("output.wav", "wb") as f:
    f.write(output_audio)
```

## Architecture

### Modular Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Modal App: speech-to-speech (Modular)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   ModelConfig (Environment Variables)  â”‚
          â”‚  ASR_MODEL | LLM_MODEL | TTS_MODEL     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚         MODEL_REGISTRY                  â”‚
          â”‚  Maps model names to implementations    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                  â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    ASR   â”‚      â”‚   LLM    â”‚      â”‚   TTS    â”‚
   â”‚          â”‚      â”‚          â”‚      â”‚          â”‚
   â”‚ Concrete â”‚      â”‚ Concrete â”‚      â”‚ Concrete â”‚
   â”‚ Models:  â”‚      â”‚ Models:  â”‚      â”‚ Models:  â”‚
   â”‚ NeMo     â”‚      â”‚ Phi3     â”‚      â”‚Chatterboxâ”‚
   â”‚ Whisper  â”‚      â”‚ Llama    â”‚      â”‚          â”‚
   â”‚          â”‚      â”‚ GPT4Mini â”‚      â”‚          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   GPU A10G          GPU A10G          GPU A10G
   @modal.method()   @modal.method()   @modal.method()
```

### Base Classes & Extensibility

Each component (ASR, LLM, TTS) is built on abstract base classes:
- **ASRModel**: Implements `load()` and `transcribe()`
- **LLMModel**: Implements `load()` and `generate()`
- **TTSModel**: Implements `load()` and `synthesize()`

**Add a new model in 3 steps:**
1. Create a class that inherits from the base (e.g., `ASRModel`)
2. Implement required methods
3. Register it: `@register_model("asr", "my_model")`

### Data Flow

```
Client (WAV)
     â”‚
     â”œâ”€â–º [Compress WAVâ†’MP3] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                          â–¼
                            Modal Remote Function
                                          â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                                         â”‚
                     â–¼                                         â–¼
          [Decompress MP3â†’WAV]                    [Load Models on First Call]
                     â”‚                                         â”‚
                     â”œâ”€â–º ASR (NeMo) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                     â”‚   Transcribe audio            â”‚        â”‚
                     â”‚                               â–¼        â”‚
                     â”‚                          â†’ LLM (Phi3)  â”‚
                     â”‚                          Generate      â”‚
                     â”‚                          response  â”€â”€â”€â”€â”¤
                     â”‚                               â”‚         â”‚
                     â”‚                               â–¼        â”‚
                     â”‚                          â†’ TTS         â”‚
                     â”‚                          (Chatterbox)  â”‚
                     â”‚                          Synthesize    â”‚
                     â”‚                               â”‚         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
                                                     â–¼         â”‚
                                          [Compress WAVâ†’MP3] â—„â”€â”˜
                                                     â”‚
                                                     â–¼
                                            Return MP3 bytes
                                                     â”‚
Client (MP3)                                        â”‚
     â”‚                                              â”‚
     â””â”€ [Decompress MP3â†’WAV] â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
  Play Audio
```

## Files

| File | Purpose |
|------|---------|
| `modular_main.py` | **Main entry point** - Modular implementation with model registry |
| `client.py` | Real-time client with VAD, compression, metrics tracking |
| `main.py` | Monolithic implementation (single container) |
| `audio_compression.py` | WAVâ†”MP3 compression utilities for network optimization |
| `requirements.txt` | Python dependencies |
| `test_compression.py` | Unit tests for audio compression |

## Environment Variables

Control model selection without code changes:

```bash
# ASR Models: nemo (default), whisper
ASR_MODEL=nemo

# LLM Models: phi3 (default), llama, gpt4omini
LLM_MODEL=phi3

# TTS Models: chatterbox (default)
TTS_MODEL=chatterbox
```

Example:
```bash
$env:ASR_MODEL="whisper"; $env:LLM_MODEL="llama"; modal deploy modular_main.py
```

## Metrics & Performance

The client tracks comprehensive metrics:

```
SESSION SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Total calls: N
Session duration: Xs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Metric                | Average      | Min          | Max
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ASR Time              | XXX.XXXms    | XXX.XXXms    | XXX.XXXms
LLM Time              | XXX.XXXms    | XXX.XXXms    | XXX.XXXms
TTS Time              | XXX.XXXms    | XXX.XXXms    | XXX.XXXms
Pipeline Total        | XXX.XXXms    | XXX.XXXms    | XXX.XXXms
Network Overhead      | XXX.XXXms    | XXX.XXXms    | XXX.XXXms
End-to-End            | XXX.XXXms    | XXX.XXXms    | XXX.XXXms
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## Audio Compression

Network-optimized audio transfer:

```python
from audio_compression import compress_wav_to_mp3, decompress_mp3_to_wav

# Compress before sending
compressed = compress_wav_to_mp3(wav_bytes, bitrate=64)

# Decompress after receiving
audio = decompress_mp3_to_wav(compressed)

# Check compression ratio
ratio = get_compression_ratio(original_bytes, compressed_bytes)
print(f"Compression ratio: {ratio:.1%}")
```

Default bitrate: 64 kbps (suitable for speech)

## Extending with New Models

### Example: Add Whisper ASR

```python
@register_model("asr", "whisper")
class WhisperASR(ASRModel):
    """OpenAI Whisper - Multilingual ASR"""
    
    def load(self):
        import whisper
        print("ðŸŽ¤ Loading Whisper...")
        self.model = whisper.load_model("base.en").cuda()
    
    def transcribe(self, audio_bytes: bytes) -> Tuple[str, float]:
        import tempfile
        import time
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            f.write(audio_bytes)
            temp_path = f.name
        
        start = time.time()
        result = self.model.transcribe(temp_path)
        elapsed = time.time() - start
        
        os.unlink(temp_path)
        return result["text"].strip(), elapsed
    
    @property
    def model_name(self) -> str:
        return "Whisper (Base)"
```

Then deploy with:
```bash
$env:ASR_MODEL="whisper"; modal deploy modular_main.py
```

### Steps to Add Your Own Model

1. **Inherit from base class**:
   ```python
   @register_model("asr", "my_model")
   class MyASR(ASRModel):
   ```

2. **Implement required methods**:
   - `load()`: Initialize model
   - `transcribe()` / `generate()` / `synthesize()`: Process data (returns tuple with result and time)
   - `model_name` property: Display name

3. **Deploy with environment variable**:
   ```bash
   $env:ASR_MODEL="my_model"; modal deploy modular_main.py
   ```

## Development

### Run Tests

```bash
# Unit tests for audio compression
pytest test_compression.py -v

# Local testing with your audio
modal run modular_main.py --audio-path input.wav

# Test deployed service
python client.py
```

### Check Logs

```bash
# Local execution
modal logs modular_main.py

# Deployed service
modal app logs speech-to-speech
```

### Monitor Resources

```bash
# List running containers
modal container list

# View app status
modal app logs speech-to-speech --tail 100
```

## Performance

| Stage | Time |
|-------|------|
| ASR | 1-3s |
| LLM | 0.5-1.5s |
| TTS | 1-2s |
| **Total (warm)** | **3-7s** |
| **Cold start** | **40-60s** |

With `keep_warm=1`, one container stays active.

## Troubleshooting

### Models not loading?

Check your Modal GPU availability:
```bash
modal netsplit
```

### High latency on first call?

First call includes model loading. Subsequent calls are faster due to `@modal.enter()` initialization.

### Audio quality issues?

- Increase TTS bitrate (default: 64 kbps)
- Try different ASR models (e.g., Whisper for robustness)
- Check microphone input level in client
